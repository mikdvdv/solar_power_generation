---
title: "Solar power generation analysis"
author: "Mikhail Davydov"
date: "7/28/2021"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Introduction
This project is the second part of a HarvardX: PH125.9x Capstone. The objective is to show data analysis techniques such as data cleaning, machine learning and data visualization. The primary steps in the project are: 

* Data description and visualization
* Cleaning the data
* Training machine learning models
* Results discussion


```{r, warning=FALSE, message=FALSE, echo=FALSE}
if(!require(e1071)) install.packages("e1071", repos = "http://cran.us.r-project.org")
if(!require(lubridate)) install.packages("lubridate", repos = "http://cran.us.r-project.org")
if(!require(ggplot2)) install.packages("ggplot2", repos = "http://cran.us.r-project.org")
if(!require(tidyverse)) install.packages("tidyverse", repos = "http://cran.us.r-project.org")
if(!require(hms)) install.packages("hms", repos = "http://cran.us.r-project.org")
if(!require(caret)) install.packages("caret", repos = "http://cran.us.r-project.org")
if(!require(gam)) install.packages("gam", repos = "http://cran.us.r-project.org")

library(e1071)
library(lubridate)
library(ggplot2)
library(tidyverse)
library(hms)
library(caret)
library(gam)

#https://www.kaggle.com/anikannal/solar-power-generation-data/download

download.file("https://github.com/mikdvdv/solar_power_generation_archive/blob/master/archive.zip?raw=true", 
              paste(getwd(), "/archive.zip", sep = ""))
# Unzipping Plant_2_Weather_Sensor_Data.csv works fine, but we get a warning for some reason, so we suppress warnings for a while. The rest of the files unzip with no issue.
defaultW <- getOption("warn")
options(warn = -1)
# Unzipping the archive
unzip("archive.zip", exdir = paste(getwd(), "/data", sep = ""))
# Enable warnings
options(warn = defaultW)
# Deleting transitory variable
rm(defaultW)

# we want results to be reproducible
set.seed(1735)

# load data for first plant
p_one_gd <- read.csv("data/Plant_1_Generation_Data.csv")
p_one_wsd <- read.csv("data/Plant_1_Weather_Sensor_Data.csv")
# transform titles of columns to lower case
names(p_one_gd) <- tolower(names(p_one_gd))
names(p_one_wsd) <- tolower(names(p_one_wsd))
# transform date_time from character type to date type.
p_one_gd$date_time <- dmy_hm(p_one_gd$date_time)
p_one_wsd$date_time <- ymd_hms(p_one_wsd$date_time)
# we have plant_id variable in p_one_gd. The variable source_key is not informative since it has only one value over the whole data set.
p_one_wsd <- p_one_wsd %>%
  select(-plant_id, -source_key)
# joining first plant's power generation data and weather sensor data
p_one <- inner_join(p_one_gd, p_one_wsd, by = "date_time")

# transforming date_time feature into two separate features - p_date and p_time
p_one <- p_one %>%
  mutate(p_date = date(date_time),
         p_time = as_hms(date_time)) %>%
  relocate(p_date, p_time, .before = plant_id) %>%
  select(-date_time)

# transform source_key so we can approach it easier
p_one$source_key <- as.numeric(as.factor(p_one$source_key))

# Arranging rows to ease our job
p_one <- p_one %>%
  arrange(source_key, p_date, p_time)

# load data for second plant
p_two_gd <- read.csv("data/Plant_2_Generation_Data.csv")
p_two_wsd <- read.csv("data/Plant_2_Weather_Sensor_Data.csv")
# transform titles of columns to lower case
names(p_two_gd) <- tolower(names(p_two_gd))
names(p_two_wsd) <- tolower(names(p_two_wsd))
# transform date_time from character type to date type.
p_two_gd$date_time <- ymd_hms(p_two_gd$date_time)
p_two_wsd$date_time <- ymd_hms(p_two_wsd$date_time)
# we have plant_id variable in p_two_gd. The variable source_key is not informative since it has only one value over the whole data set.
p_two_wsd <- p_two_wsd %>%
  select(-plant_id, -source_key)
# joining second plant's power generation data and weather sensor data
p_two <- inner_join(p_two_gd, p_two_wsd, by = "date_time")

# transforming date_time feature into two separate features - p_date and p_time
p_two <- p_two %>%
  mutate(p_date = date(date_time),
         p_time = as_hms(date_time)) %>%
  relocate(p_date, p_time, .before = plant_id) %>%
  select(-date_time)

# Transforming source_key like we did with p_one, only this time we add a constant to merge it with p_one
p_two$source_key <- as.numeric(as.factor(p_two$source_key)) + 
  length(unique(p_one$source_key))

# Arranging rows to ease our job
p_two <- p_two %>%
  arrange(source_key, p_date, p_time)

# Deleting redundant variables
rm(p_one_gd, p_one_wsd, p_two_gd, p_two_wsd)

# Merging two data sets into one
plants <- rbind(p_one, p_two)
rm(p_one, p_two)
# Setting plant_id as a factor. We need factors for classification algorithms
plants$plant_id <- as.factor(plants$plant_id)
# Splitting data on training and test set
test_index <- createDataPartition(y = plants$plant_id, times = 1, p = 0.2, 
                                  list = FALSE)
train_set <- plants[-test_index,]
test_set <- plants[test_index,]
rm(plants, test_index)

```


## Data set
The used data set has been taken at <https://www.kaggle.com/anikannal/solar-power-generation-data>. According to the description provided by the author, this data has been gathered at two solar power plants in India over 34 days. Original files from Kaggle have been merged into one data set. date_time feature has been separated into p_date and p_time. Then we have split the dataset into training (80%) and test set (20%). I have chosen this split because we need lots of data to train a model, and 20% of a data set is enough to evaluate it, given this enormous dataset we have.

```{r, echo=FALSE}
head(train_set)
```

\pagebreak
The variables in the dataset are:

* p_date, p_time - date and time observation had been made.
* plant_id - solar power plant id. We have two of them.
* source_key - is the inverter id. Each inverter has a bunch of solar panels. I am going to call it a solar panel for the sake of convenience. 
* ac_power, dc_power - show the amount of ac, dc power being produced at 15-minute intervals. 
* daily_yield is a cumulative sum of power generated on that day until that point in time. 
* total_yield is the total yield for the inverter until that point in time. 
* ambient_temperature is the outside temperature at the power plant. 
* module_temperature is the temperature of a solar panel. 
* irradiation is the amount of solar irradiation for the 15-minute interval.
The challenge is to predict at which power plant an observation has been made.

# Analysis
Let's take a look at the first plant. Do the solar panels have the same performance?

```{r, echo=FALSE}
# Do the solar panels have the same performance?
train_set %>%
  filter(plant_id == 4135001) %>%
  group_by(source_key) %>%
  summarise(performance = sum(dc_power + ac_power)) %>%
  ggplot(aes(source_key, performance)) + geom_point(color = "steelblue")
```

We have two outliers. 

\pagebreak
Shall we take a look at the performance over time.

```{r, echo=FALSE}
train_set %>%
  filter(plant_id == 4135001) %>%
  aggregate(cbind(dc_power, ac_power) ~ source_key + p_date, FUN = sum, data = .) %>%
  mutate(performance = dc_power + ac_power) %>%
  ggplot(aes(p_date, performance, color = as.factor(source_key))) + geom_point() + 
  geom_path()
```

The trend is present, but it is too many panels on the plot. 

\pagebreak
How about taking a subset by source_key 

```{r, echo=FALSE}
train_set %>%
  filter(plant_id == 4135001) %>%
  filter(source_key < 6) %>%
  aggregate(cbind(dc_power, ac_power) ~ source_key + p_date, FUN = sum, data = .) %>%
  mutate(performance = dc_power + ac_power) %>%
  ggplot(aes(p_date, performance, color = as.factor(source_key))) + geom_point() + 
  geom_path()
```

We can see that some panels work better than others. Maybe because solar panels have a different angle to the sun, or variation is the norm regarding solar panel manufacturing. Or maybe new panels work better than old ones.

\pagebreak
Would you like to take a look at the second plant.

```{r, echo=FALSE}
train_set %>%
  filter(plant_id == 4136001) %>%
  aggregate(cbind(dc_power, ac_power) ~ source_key + p_date, FUN = sum, data = .) %>%
  mutate(performance = dc_power + ac_power) %>%
  ggplot(aes(p_date, performance, color = as.factor(source_key))) + geom_point() + 
  geom_path()
```

A lot messier than the data from the first plant. Let's dive into it. 

\pagebreak
The plot for the first five panels is

```{r, echo=FALSE}
train_set %>%
  filter(plant_id == 4136001) %>%
  filter(source_key < 23 + 5) %>%
  aggregate(cbind(dc_power, ac_power) ~ source_key + p_date, FUN = sum, data = .) %>%
  mutate(performance = dc_power + ac_power) %>%
  ggplot(aes(p_date, performance, color = as.factor(source_key))) + geom_point() + 
  geom_path()
```

Maybe some panels didn't generate power during some periods. That is why the lines on the plot don't move along. Note, panel #27 doesn't have any observations from May 20th through May 29th.

\pagebreak
What is the correlation between ac_power and irradiation.

```{r, warning=FALSE, message=FALSE, echo=FALSE}
ggplot(train_set, aes(irradiation, ac_power)) + geom_point(color = "steelblue")
```

We can see the trend, but at the same time, there are lots of outliers. Why not to take a look at some of them.

```{r}
head(train_set %>%
       filter(irradiation > 0.6 & ac_power > 0 & ac_power < 250))
```

You may notice a total_yield drop at 2020-05-19	at 10:15:00. Earlier observations for source_key #23 give you a greater total_yield, which shouldn't occur by design. 

\pagebreak
Taking a closer look

```{r}
head(train_set %>%
       filter(p_date == ymd("2020-05-19") & p_time > hms(60 * 60 * 9) & source_key == 23), 10)
```

Not only total_yield has dropped but also daily_yield. Remember, these numbers are cumulative. The panel turned off later in the day. This type of total_yield drop is all over the dataset. We need to cut these observations since they represent nothing but noise. To do so, we are going to compare each observation's total_yield with the oldest total_yield for that particular source_key. We can do so because total_yield drops way below the oldest one.

```{r}
# This function cuts troublesome observations out of a data frame. malf_list is a result of a sapply() function, so it is a list of vectors. Every vector represents something like source_key, for example. The name stands for "cut malfunction".
cut_malf <- function(malf_list, data_set){
  # Creating a variable
  malf_vec <- c(0)
  # Transforming a list of vectors in one huge vector
  for (xx in 1:length(malf_list)){
    malf_vec <- c(malf_vec, malf_list[[xx]])
  }
  # We need to delete the first value from the vector, since it had been created to create the vector itself.
  malf_vec <- malf_vec[2: length(malf_vec)]
  # Deleting broken observations from the data set.
  data_set %>%
    slice(which(!malf_vec))
}

# This function deletes observations with total_yield drop. The name stands for "delete total yield drop"
del_ttl_yld_drp <- function(data_set){
  # Creating a list of vectors to tackle total_yield drop. Every vector matches with source_key.
  malf_list1 <- sapply(unique(data_set$source_key), function(xx){
    # Taking the initial total_yield
    init_yield <- head(data_set %>%
            filter(source_key == xx), 1)$total_yield
    
    # Comparing total_yield with initial yield we have got earlier
    data_set %>%
      filter(source_key == xx) %>%
      mutate(malf = ifelse(total_yield < init_yield, 1, 0)) %>%
      pull(malf)
  })
  # Cut observations with wrong total_yield. And return clean data set as a result of the function
  cut_malf(malf_list1, data_set)
}
```

We will clean the data set and, at the same time, will see how many observations are damaged.

```{r}
# Number of observations before cleaning
n_before <- nrow(train_set)
# Cutting total yield drops
train_set <- del_ttl_yld_drp(train_set)
# The number of damaged observations is
n_before - nrow(train_set)
```
```{r, echo=FALSE}
rm(n_before)
```

Enough to make a mess.

```{r, warning=FALSE, message=FALSE, echo = FALSE}
ggplot(train_set, aes(irradiation, ac_power)) + geom_point(color = "steelblue")
```

The plot looks better, but we still have some issues here.

\pagebreak
```{r}
head(train_set %>%
       filter(irradiation > 0.6 & ac_power > 0 & ac_power < 250))
```

I pick 3rd observation to explore.

```{r}
head(train_set %>%
       filter(p_date == ymd("2020-05-16") & p_time > hms(60 * 60 * 9) & source_key == 23), 10)
```

This observation had been made right before downtime. It turns out measurements right before or after pause often have low productivity. My guess is - a panel didn't work the whole 15-minute interval, so the observation is not representative. We are going to eliminate peripherals, but we are going to do it smartly. We will cut only showing an anomaly in the correlation between irradiation and ac_power because lots of peripheral observations have been made at the beginning or the end of the day. First, we mark peripherals with "1" in the corresponding variable. The rest of the observations are "0".

```{r}
# This function creates a variable showing a switch from zero to some number or vice versa in the ac_power feature. The name stands for "add switch variable based on ac_power"
add_swtch_vr_ac <- function(data_set){
  # Creating a feature representing switches from zero to non-zero number in ac_power. First non-zero observations after zeros get 1, the rest of observations get 0.
  data_set <- data_set %>%
    # Creating a feature showing whether ac_power has zero or non-zero value.
    mutate(swtch_f_z_ac = ifelse(ac_power == 0, 1, 2)) %>%
    # Comparing each observation with its previous one. If the two observations are the same - the sum will result in an even number (either two or four). If they are not - the sum going to be an odd number (three). Then we divide the sum by two and take the reminder
    mutate(swtch_f_z_ac = (swtch_f_z_ac + c(1, swtch_f_z_ac[1:(n()-1)])) %% 2) %>%
    # If current ac_power == 0 and in the previous observation ac_power has a non-zero value, the line above returns "1". We don't need that. So we set zeros to observations with ac_power == 0.
    mutate(swtch_f_z_ac = ifelse(ac_power == 0, 0, swtch_f_z_ac)) %>%
    relocate(swtch_f_z_ac, .after = ac_power)
  # Creating a feature representing switches from a non-zero value to zero in ac_power. Last non-zero observations before zeros get 1, the rest of observations get 0.
  data_set <- data_set %>%
    # Creating a feature showing weather ac_power has zero or non-zero value.
    mutate(swtch_f_n_ac = ifelse(ac_power == 0, 1, 2)) %>%
    # Comparing each observation with its next one. If the two observations are the same - the sum will result in an even number (either two or four). If they are not - the sum going to be an odd number (three). Then we divide the sum by two and take the reminder
    mutate(swtch_f_n_ac = (swtch_f_n_ac + c(swtch_f_n_ac[2:(n())], 0)) %% 2) %>%
    # If current ac_power == 0 and in the next observation ac_power has a non-zero value, the line above returns "1". We don't need that. So we set zeros to observations with ac_power == 0.
    mutate(swtch_f_n_ac = ifelse(ac_power == 0, 0, swtch_f_n_ac)) %>%
    relocate(swtch_f_n_ac, .after = ac_power)
  # Combine switches from zero to non-zero with switches from non-zero to zero.
  data_set %>%
    mutate(swtch_ac = swtch_f_n_ac + swtch_f_z_ac) %>%
    relocate(swtch_ac, .after = ac_power) %>%
    select(-swtch_f_n_ac, -swtch_f_z_ac)
}
# Marking peripheral observations
train_set <- add_swtch_vr_ac(train_set)
```

The next thing we are going to do is trying to estimate if ac_power is reasonable for irradiation a solar panel gets. To do that, we will bin irradiation and compute the mode value of ac_power for each bin. But first, we need to define the mode function since R doesn't have one.

```{r}
# Finding mode. https://www.tutorialspoint.com/r/r_mean_median_mode.htm .According to https://courses.edx.org/courses/course-v1:HarvardX+PH125.9x+1T2021/discussion/forum/e3f44cffce1cc3ed3b72f06e629fd7b3a5d895c8/threads/60ae7dfce136290450b23d5b I can do that.
getmode <- function(v) {
  # Getting unique values from the vector v
  uniqv <- unique(v)
  # Getting the most common value
  uniqv[which.max(tabulate(match(v, uniqv)))]
}
# This function bins irradiation feature and computes mode of ac_power and dc_power for each bin
get_pwr_md_irr <- function(data_set, n_bins){
  # Binning irradiation feature, so we can find the mode of ac_power and dc_power for each bin
  data_set <- data_set %>%
    mutate(irrad_bin = round(irradiation * n_bins))
  
  # Finding the mode of ac_power and dc_power
  data_set %>%
    # Deleting zeros, so they don't spoil the mode. We don't need them anyway
    slice(which(ac_power != 0 & dc_power != 0)) %>% 
    group_by(irrad_bin) %>%
    summarise(irrad_mode_ac = getmode(ac_power), irrad_mode_dc = getmode(dc_power))
}

```

We are ready to define a function that cuts peripheral observations with low productivity.

```{r}
cut_swtch_malf <- function(data_set, irr_bin, num_irr_bin){
    # Binning irradiation feature, so we can join the data set with irr_bin
  data_set <- data_set %>%
    mutate(irrad_bin = round(irradiation * num_irr_bin))
  # Adding mode of ac_power and dc_power to the data set and find the distance between ac_power and the mode of ac_power. Same for dc_power
  data_set <- left_join(data_set, irr_bin, by = "irrad_bin") %>%
    mutate(mode_dist_ac = irrad_mode_ac - ac_power,
           mode_dist_dc = irrad_mode_dc - dc_power)


  
  # Cutting off observations where c_power significantly differs from its mode given irradiation and the observation has been made right before or after the panel didn't work
  data_set %>%
    filter(irradiation > 0) %>%
    # We measure the significance of the difference as sqrt() function because when irradiation is low, 200 is a lot, but when irradiation is high, 200 is ok. So we need a curve, and sqrt() fits nice. The multiplier for sqrt() is arbitrary, has been chosen by trial and error
    slice(which(!(abs(mode_dist_ac) > (sqrt(irradiation) * 400) & swtch_ac != 0))) %>%
    # Deleting transitory features. 
    select(-mode_dist_ac, -mode_dist_dc, -irrad_mode_ac, -irrad_mode_dc)
}
```

Let's see what we have done.

```{r}
# Number of observations before cleaning
n_before <- nrow(train_set)
# Number of observations where irradiation == 0 
n_zeros <- length(which(train_set$irradiation == 0))

# This number defines the number(not exact) of irradiation bins.
num_irr_bin <- 18
# Getting ac_power mode for each irradiation bin
irr_bin <- get_pwr_md_irr(train_set, num_irr_bin)
# Cutting peripheral observations with low productivity
train_set <- cut_swtch_malf(train_set, irr_bin, num_irr_bin)

# The number of damaged observations is
n_before - n_zeros - nrow(train_set)
```
```{r, echo=FALSE}
rm(n_before, n_zeros)
```


```{r, echo=FALSE}
ggplot(train_set, aes(irradiation, ac_power)) + geom_point(color = "steelblue")
```

Lot better. Given that we cut so few values.

\pagebreak
What else can we find? Let's throw some colours on the plot above.

```{r, echo=FALSE}
ggplot(train_set, aes(irradiation, ac_power, color = module_temperature)) + geom_point() + scale_color_gradient(low = "blue", high = "red")

```

Outliers tend to have high module_temperature.

\pagebreak
At first, I thought panels are overheating and stop working. It turns out not to be true. Watch it from another angle.

```{r, echo=FALSE}
train_set %>%
  ggplot(aes(color = ac_power, x = irradiation, y = module_temperature)) + geom_point() + scale_color_gradient(low = "blue", high = "red")

```

If my assumption about overheating was true, then we would have an "overheating" line with the majority of the dots being blue above it - which is not the case. But for some reason, ac_power tends to drop at high irradiation values. 

\pagebreak
One may think we don't have many observations where irradiation is in the segment from 0.15 to 0.45 - that is why we don't have many outliers there. It is not true either.

```{r, warning=FALSE, message=FALSE, echo=FALSE}
train_set %>%
  ggplot(aes(irradiation)) + geom_histogram()
```

\pagebreak
The same plot with a scope on high irradiation and non-zero ac_power.

```{r, echo=FALSE}
train_set %>%
  filter( ac_power > 0 & irradiation > 0.6 & module_temperature > 45) %>%
  ggplot(aes(color = ac_power, x = irradiation, y = module_temperature)) + geom_point() + scale_color_gradient(low = "blue", high = "red")


```

Non-zero ac_power values are present. 
I don't see any sound reason to delete the rest of the outliers. If you disagree, you can cut them with functions I have left for you. You will find them commented in the code.R file. I am not going to explain how they work since the way they work is pretty much the same as irr_bin() and cut_swtch_malf().

\pagebreak
By the way, notice most of the outliers are from the second power plant.

```{r, echo=FALSE}
ggplot(train_set, aes(irradiation, ac_power, color = plant_id)) + geom_point()
```

At this point, I see the data as clean.

\pagebreak
Recall, we are trying to predict at which power plant an observation has been made. Look at the plot.

```{r, echo=FALSE}
ggplot(train_set, aes(irradiation, dc_power, color = plant_id)) + geom_point()
```

Power plants produce different amounts of dc_power given the same irradiation. So we can use these two variables to make a prediction. I see two possible ways to solve this classification problem. We can set a boundary as a line, or we can predict based on the values neighbours have. We will implement three algorithms: lda, knn and gemLoess

First, we clean the test set the way we have done the training set.

```{r}
test_set <- add_swtch_vr_ac(test_set)
test_set <- del_ttl_yld_drp(test_set)
test_set <- cut_swtch_malf(test_set, irr_bin, num_irr_bin)
```

Then we cut features we don't need.

```{r}
train_set <- train_set %>%
  select(plant_id, dc_power, irradiation)

test_set <- test_set %>%
  select(plant_id, dc_power, irradiation)
```

# Results


## Training Linear discriminant analysis

```{r}
# lda
model_lda <- train(plant_id ~ ., data = train_set, method = "lda")
```

\pagebreak
We can make a probability distribution heatmap to see what the algorithm predicts.

```{r}
# Constructing the values' grid
pow_grid <- rep(seq(0, max(train_set$dc_power), length.out = 100), 100)
irr_grid <- rep(seq(0, max(train_set$irradiation), length.out = 100), each = 100)
# Combining it into a data frame to predict from
prob_dist <- data.frame(irradiation = irr_grid, dc_power = pow_grid)
# Computing probabilities of getting the second power plant as a result
lda_prob_dist <- predict(model_lda, prob_dist, type = "prob")[,2]
# Drawing the probabilities
prob_dist %>%
  mutate(p_dist = lda_prob_dist) %>%
  ggplot(aes(irradiation, dc_power, p_dist, fill = p_dist)) + geom_raster() +
  scale_fill_gradientn(colors=c("#F8766D","white","#00BFC4")) +
  labs(title = "Probability distribution heatmap. lda")
```



## Training knn
I have tuned the algorithm in advance. With k = 1, we have the best accuracy. Generally, k = 1 may result in overfitting. 
```{r}
# knn
model_knn <- train(plant_id ~ ., data = train_set, method = "knn",
                   tuneGrid = data.frame(k = 1))
```

```{r, echo=FALSE}
# Computing probabilities of getting the second power plant as a result
knn_prob_dist <- predict(model_knn, prob_dist, type = "prob")[,2]
# Drawing the probabilities
prob_dist %>%
  mutate(p_dist = knn_prob_dist) %>%
  ggplot(aes(irradiation, dc_power, p_dist, fill = p_dist)) + geom_raster() +
  scale_fill_gradientn(colors=c("#F8766D","white","#00BFC4")) +
  labs(title = "Probability distribution heatmap. knn. k = 1")
```

Every cell in the grid either has a probability of 0 or 1. There is Nothing in between. As far as I can tell, it looks like overfitting. Let's try k = 7

```{r, echo=FALSE}
# knn
model_knn <- train(plant_id ~ ., data = train_set, method = "knn",
                   tuneGrid = data.frame(k = 7))
```

```{r, echo=FALSE}
# Computing probabilities of getting the second power plant as a result
knn_prob_dist <- predict(model_knn, prob_dist, type = "prob")[,2]
# Drawing the probabilities
prob_dist %>%
  mutate(p_dist = knn_prob_dist) %>%
  ggplot(aes(irradiation, dc_power, p_dist, fill = p_dist)) + geom_raster() +
  scale_fill_gradientn(colors=c("#F8766D","white","#00BFC4")) +
  labs(title = "Probability distribution heatmap. knn. k = 7")
```

Too much smoothing. Let's try k = 3.

```{r, echo=FALSE}
# knn
model_knn <- train(plant_id ~ ., data = train_set, method = "knn",
                   tuneGrid = data.frame(k = 3))
```

```{r, echo=FALSE}
# Computing probabilities of getting the second power plant as a result
knn_prob_dist <- predict(model_knn, prob_dist, type = "prob")[,2]
# Drawing the probabilities
prob_dist %>%
  mutate(p_dist = knn_prob_dist) %>%
  ggplot(aes(irradiation, dc_power, p_dist, fill = p_dist)) + geom_raster() +
  scale_fill_gradientn(colors=c("#F8766D","white","#00BFC4")) +
  labs(title = "Probability distribution heatmap. knn. k = 3")
```

This tune is the most reasonable one, in my opinion.

## Training gamLoess

This algorithm is computationally expensive. We will take 5% of the training set to train it. I have already tuned parameters, so we are going to train the algorithm using the best tune. Tuning code you can find in code.R

```{r, warning=FALSE, message=FALSE}
# Creating a subset of data, since the algorithm is computationally expensive
g_train_index <- createDataPartition(y = train_set$plant_id, times = 1, p = 0.05, 
                                  list = FALSE)
g_train_set <- train_set[g_train_index,]


# gamLoess
grid <- expand.grid(span = 0.2, degree = 1)
model_gam <- train(plant_id ~ ., data = g_train_set, method = "gamLoess", 
                   tuneGrid = grid)
```

```{r, warning=FALSE, message=FALSE, echo=FALSE}
# Computing probabilities of getting the second power plant as a result
gam_prob_dist <- predict(model_gam, prob_dist, type = "prob")[,2]
prob_dist %>%
  mutate(p_dist = gam_prob_dist) %>%
  ggplot(aes(irradiation, dc_power, p_dist, fill = p_dist)) + geom_raster() +
  scale_fill_gradientn(colors=c("#F8766D","white","#00BFC4")) +
  labs(title = "Probability distribution heatmap. gamLoess. span = 0.2, degree = 1")
```

It looks like overfitting. No wonder. We have used less than 3000 observations to train the algorithm. I am going to change the tuning parameter.

```{r, warning=FALSE, message=FALSE, echo=FALSE}
# gamLoess
grid <- expand.grid(span = 0.5, degree = 1)
model_gam <- train(plant_id ~ ., data = g_train_set, method = "gamLoess", 
                   tuneGrid = grid)
```
```{r, warning=FALSE, message=FALSE, echo=FALSE}
# Computing probabilities of getting the second power plant as a result
gam_prob_dist <- predict(model_gam, prob_dist, type = "prob")[,2]
prob_dist %>%
  mutate(p_dist = gam_prob_dist) %>%
  ggplot(aes(irradiation, dc_power, p_dist, fill = p_dist)) + geom_raster() +
  scale_fill_gradientn(colors=c("#F8766D","white","#00BFC4")) +
  labs(title = "Probability distribution heatmap. gamLoess. span = 0.5, degree = 1")
```

I like this one. How about compare these algorithms in terms of accuracy.

```{r, warning=FALSE, message=FALSE}

y_hat_lda <- predict(model_lda, test_set, type = "raw")
y_hat_knn <- predict(model_knn, test_set, type = "raw")
y_hat_gam <- predict(model_gam, test_set, type = "raw")

tibble(lda = confusionMatrix(y_hat_lda, test_set$plant_id)$overall["Accuracy"],
       knn = confusionMatrix(y_hat_knn, test_set$plant_id)$overall["Accuracy"],
       gam = confusionMatrix(y_hat_gam, test_set$plant_id)$overall["Accuracy"])
```

There is no significant difference in algorithms' accuracy. My choice for this task is lda. It is fast, accurate and uses all the data available. While in reality, lda is not as prevalent because it is almost impossible to draw a straight line to get something beneficial in complicated cases.

# Conclusion
We have cleaned the data in a responsible way - which means every change we made is reasonable. We predicted the outcomes with three different algorithms and discussed the results. We have seen two ways to classify - classification by neighbours, which manifested in knn and its smoothed version - gemLoess and drawing a line - lda in our case. We have seen flaws and advantages of each algorithm in performance, computational complexity and ability to capture the trend. It is amusing the most basic and fastest model of all is the optimal one. The next step in the pipeline is to start using all CPU cores, which may open the door for training gamLoess on all the data we have. Also, it is crucial to investigate turn-offs of the panels when irradiation is high. If we solve this issue, then the electricity going to be more accessible.


